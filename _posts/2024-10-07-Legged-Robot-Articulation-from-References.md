---
layout:     post
title:      "Legged Robot Articulation from Various References"
subtitle:   "Study the past if you would define the future."
date:       2024-10-11 12:00:00
author:     "Yubin"
header-img: "img/Headers/wallhaven-2yxdxx.jpg"
mathjax: true
catalog: true
scholar:
    bibliography: rinko_2.bib
tags:
    - Reinforcement Learning
    - Robotics
    - Deep Learning
---

This is a summary of my presentation in my second Rinko at the University of Tokyo. In this Rinko, I chose the topic of **Learning-Based Methods for Legged Robot Articulation from Various References**.

***

# Abstract
Legged robots have demonstrated remarkable potential in navigating complex and challenging environments, achieving significant progress and success in various applications over recent years.
Recent advancements in robot hardware and reinforcement learning technologies have enabled these robots to exhibit agile locomotion within complex environments compared with traditional optimal control-based methods. 
This survey delves into the advancements in learning-based methods for robot articulation, highlighting their capability to train lifelike, agile robots that integrate prior knowledge from reference kinematic data while remaining environment-aware. Empirical studies showcase the maneuverability of quadruped robots across diverse terrains, reflecting their excellent performance and adaptability.


# Introduction
Legged robots have shown significant potential to navigate complex and challenging environments, and we have witnessed explosive progress and substantial success in numerous applications over the past few years {% cite hendra2023quadruped %}, {% cite defazio2023seeing %}. The empirical demonstration of quadruped robots' maneuverability across diverse terrains establishes their commendable performance. Various methodologies are utilized to enable quadruped, bipedal, and humanoid robots to navigate diverse terrains, with a notable focus in recent years on learning-based approaches {% cite kumar2021rma %}{% cite zhuang2023robot %}{% cite haarnoja2024learning %}{% cite agarwal2023legged %}{% cite cheng2024extreme %}{% cite duan2024learning %}. The following figure shows examples of legged robots articulating in various environments.

<figure>
    <img width="50%" align="middle" src="/img/Notes/2024-10/real_robot.png" style="margin-top: 0px; margin-bottom: 5px"/>
    <div style="font-size: 12px; text-align: center; margin-top: 0px;">
    Examples of notable learned behaviors for quadruped robots executed in the real world. {% cite ha2024learning %}
    </div>
</figure>

To enable quadruped robots to perform agile maneuvers akin to their biological counterparts, such as dogs or cats, it is crucial to thoroughly explore and understand the robot's action space. Inspired by curriculum learning, an incremental hardness training strategy is used to adapt the difficulty level for each terrain type individually {% cite rudin2022learning %}. With this curriculum learning strategy and sophisticated reward-engineering, robots can learn locomotion skills in challenging environments {% cite zhuang2023robot %}{% cite cheng2024extreme %} or complete loco-manipulation tasks using their bodies and feet {% cite cheng2023legs %}. In such research, a teacher-student training framework is often utilized to transfer knowledge from the teacher to the student network. This training process can be considered a form of behavior cloning, where the robot learns to imitate the teacher's motion.

However, the teacher's motion can originate from various sources, such as motion capture data {% cite peng2020learning %}, videos {% cite zhang2023slomo %}, Model Predictive Control (MPC) planners {% cite jenelten2024dtc %}, diffusion method-based motion generators {% cite xie2023movement %}, or even data generated by a Large Language Model (LLM) {% cite tang2023saytap %}. The following table illustrates the comparison of different reference motion sources. We list the advantages and disadvantages of each method in the table to highlight that each method has its own unique characteristics.

<figure>
    <img width="90%" align="middle" src="/img/Notes/2024-10/table.png" style="margin-top: 0px; margin-bottom: 5px"/>
    <div style="font-size: 12px; text-align: center; margin-top: 0px;">
    Comparison of different reference motion sources. <b>Kinetic</b> means whether the reference motion is kinetic data. <b>Satisfies Robot's Morphology</b> means the reference motion can be directly applied to robot without retargetting or mapping. <b>Non-Craftwork</b> refers to reference motions that are not being meticulously designed or hand-crafted directly or indirectly.
    </div>
</figure>

In this survey, we will investigate different methods for applying reference motions to robots and discuss their distinctions. In summary, we intend to introduce and discuss the following topics:
- How to adapt the reference motion to the robot's morphology and train the robot to imitate the motion.
- How to learn a generalized motion prior that can be applied to various motions.
- How to generate new actions based on the collected dataset, whether it is only kinematic data or not.
- How to embed the text command into the robot's motion reference using LLM.

We would like to first introduce some necessary backgrounds of learning-based articulation methods, including the Generative Adversarial Imitation Learning (GAIL) and generation model, which are the basic knowledge of following sections. Then the subsequent two sections will detail these topics. The final chapter will present the conclusion and outline avenues for future research.



# Background
## Generative Adversarial Imitation Learning

Imitation learning, also known as behavioral cloning, is used to train a policy to imitate an expert's behavior, which may originally come from the automated driving area {% cite bojarski2016end %}. Given a dataset with observations and demonstrations, the policy is trained to minimize the difference between the output of the policy and the demonstration to imitate the expert.

However, imitation learning will suffer a drifting problem if only a small amount of demonstrations are available. Moreover, it is hard to derive the demonstrated action commands about motion data captured by real animals. To solve this problem, generative adversarial imitation learning (GAIL) {% cite ho2016generative %} is utilized. GAIL is a method that combines imitation learning and generative adversarial networks (GANs) by learning a discriminator $D$ to measure the similarity between policy $\pi_\theta$ and expert trajectories $\tau_E \sim\pi_E$ using:

$$
    \mathcal{L}_{\text{GAIL}} = \mathbb{E}_{\tau_i}[\log(D_w(s,a))] + \mathbb{E}_{\tau_E}[\log(1-D_w(s,a))] 
$$

, where $w$ denotes the parameters of discriminator $D$. The policy is then trained using the reinforcement learning with rewards specified by:

$$
    r_t = \log(D_{w}(s_t,a_t))
$$

Generally speaking, this reward function encourages the policy to generate actions that are not able to be distinguished from the demonstrations by the discriminator. The following algorithm shows the detailed training process of GAIL, which can be seen as a modern approach to imitation learning that builds on the foundational ideas of inverse reinforcement learning (IRL) but leverages adversarial training to directly learn policies.

<figure>
    <img width="70%" align="middle" src="/img/Notes/2024-10/gail.png" style="margin-top: 0px; margin-bottom: 5px"/>
    <div style="font-size: 12px; text-align: center; margin-top: 0px;">
    Generative adversarial imitation learning (GAIL) {% cite ho2016generative %}
    </div>
</figure>

## Variational Auto-Encoder
Variational Auto-Encoders (VAEs) are a class of generative models that have gained substantial attention in the field of machine learning and artificial intelligence for their ability to learn latent representations of data and generate new data samples that are statistically similar to a given dataset. Introduced by Kingma and Welling in 2013, VAEs combine principles from Bayesian inference and neural networks, providing an elegant framework for unsupervised learning {% cite kingma2013auto %}.

From a mathematical perspective, VAEs aim to maximize the Evidence Lower Bound (ELBO), which serves as a variational approximation to the true data likelihood. Mathematically, the ELBO for a VAE can be expressed as follows:

$$
    \mathcal{L}_{\text{VAE}}(\theta, \phi; \textbf{x}) = \mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})} [\log p_\theta(\textbf{x}|\textbf{z})] - \text{KL}(q_\phi(\textbf{z}|\textbf{x}) || p(\textbf{z}))
$$

, where 
$q_\phi(\textbf{z}|\textbf{x})$
represents the encoder network approximating the posterior distribution of latent variables given the data, $p_\theta(\textbf{x}|\textbf{z})$ represents the decoder network probabilistically reconstructing the data, and $p(\textbf{z})$ is the prior distribution over the latent variables. 

Based on VAE architecture, researchers proposed Vector Quantized Variational Auto-Encoder (VQ-VAE) {% cite van2017neural %} in order to address some limitations of traditional VAE related to their latent space representations. At a high level, VQ-VAE modifies the standard VAE architecture by incorporating a vector quantization layer, which forces the encoder to output discrete latent codes rather than continuous ones to capture the underlying structure of the input data more effectively. 

In VQ-VAE, the encoder maps the input to one latent embedding and the decoder maps the embedding back into an output to recover the input. The VQ-VAE training loss consists of a reconstruction loss and a commitment loss as denoted below:

$$
        \mathcal{L}_{\text{VQ-VAE}}(\theta, \phi; \textbf{x}) =\mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})} [\log p_\theta(\textbf{x}|\textbf{z})]
        + \left\|sg\left[z^e(\textbf{x})\right]-\textbf{e}\right\|_2^2
        + \beta\left\|z^e(\textbf{x})-{sg}[\textbf{e}]\right\|_2^2
$$

, where the reconstruction loss for the input data $x$ is

$$
\mathbb{E}_{q_\phi(\textbf{z}|\textbf{x})}[\log p_\theta(\textbf{x}|\textbf{z})]
$$

; $sg$ is an operator to stop the gradients; $z^e(\textbf{x})$ is the output of the encoder; $\textbf{e}$ is the nearest embedding from the discrete latent embeddings to $z^e(x)$; and the hyperparameter $\beta$ balances the last two terms.


## Diffusion Model and Applications

Introduced by Ho et al. in 2020, the diffusion model leverages the principles of Langevin dynamics to model the data distribution and generate new samples that are statistically similar to the original dataset {% cite ho2020denoising %}. From the perspective of VAE's extension, the diffusion models are incremental updates where the assembly of the whole gives us the encoder-decoder structure {% cite chan2024tutorial %}.  We will not dive into the mathematical derivation of the diffusion model in this survey. The loss function during training is defined as:

$$
    \mathcal{L}_{\text{DM}}(\theta, t; \textbf{x}) = \mathbb{E}_{t, \textbf{x},\epsilon}[||\epsilon_t - \epsilon_{\theta}(\textbf{x}_t, t)||_2^2]
$$

to minimize the difference between the predicted noise and the added noise, where $\epsilon_t$ is the noise added to the input data at time $t$, and $\epsilon_{\theta}(\textbf{x}_t, t)$ is the noise predicted by the model. And during sampling time, the model recovers images from standard Gaussian noise in $T$ time steps by:

$$
    \textbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}(\textbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\overline{\alpha}_t}}\epsilon_\theta(\textbf{x}_t, t)) + \sigma_t\textbf{z}
$$

, where the $\alpha_t$ and $\overline{\alpha}_t$ are the parameters to control the diffusion process, $\sigma_t$ is a variance constant, and $\textbf{z}$ is a standard Gaussian noise.

The diffusion model has been applied to various fields, such as image generation {% cite ho2020denoising %}, motion generation {% cite tevet2023human %}, and even robot control {% cite chi2023diffusion %}. In motion diffusion model (MDM) {% cite tevet2023human %}, the model is fed a motion sequence $\textbf{x}^{1:N}_t$ of length N in a noising step t, as well as $t$ itself and a CLIP {% cite radford2021learning %} based textual embedding $c$. In each sampling step, the transformer encoder predicts the final clean motion $\hat{\textbf{x}}_t^{1:N}$.
The model is trained using the loss function before with an additional geometric loss term for regularization.

# Learning from captured data
## Adapt MoCap data to robot

Several types of research have been proposed to imitate reference motions in physics-based animation generation {% cite zhang2018mode %}{% cite  liu2017learning %}{% cite  yao2022controlvae %} However, due to the gap between simulation and the real world, it is hard to directly apply these methods to real robots. 
Some researchers use optimal control methods to retarget the mocap data or videos with noise due to the unpredictability of creature behavior to robot {% cite kang2021animal %}{% cite grandia2023doc %}{% cite zhang2023slomo %}.
With recent advancements in reinforcement learning, in {% cite peng2020learning %}, the authors propose a method to adapt the reference motion to the robot with three stages: motion retargeting, motion imitation, and domain adaptation.


**Motion Retargeting:** 
In motion retargeting, the source motions are retargeted to the robot's morphology using inverse kinematics (IK). A set of source key points from the subject's body are paired with corresponding on the robot, which is illustrated in the following figure. With the robot's pose $\textbf{q}_t$ and the IK solver $\textbf{x}_i(\textbf{q}_t)$ for each point $i$, the source motion can be retargeted to the robot's morphology by:

$$
    \arg\min_{\textbf{q}_{0:T}} \sum_t\sum_i ||\hat{\textbf{x}}_i(t)-\textbf{x}_i(\textbf{q}_t)||^2
    + (\bar{\textbf{q}} - \textbf{q}_t)^T\textbf{W}(\bar{\textbf{q}} - \textbf{q}_t)
$$

, where $T$ is the time sequence length, and $\textbf{W}$ denotes a diagonal matrix with weights for each joint to let the robot remain similar to the default pose $\bar{\textbf{q}}$ as a regularization term.

**Motion Imitation:** 
During motion imitation, the robot is trained to imitate the retargeted motion in a reinforcement learning approach. The policy $\pi_\theta$ is trained to minimize the difference between the output of the policy and the retargeted motion using reward $r_t$:
$r_t = \sum_iw^ir_t^i$
, where $i$ denotes different kinds of rewards designed by the author to encourage the robot to minimize the tracking errors on both root and joint positions and velocities and $w^i$ is the weight for each reward. The following table shows the detailed reward design, which is similar to the same tasks in computer graphics named DeepMimic {% cite peng2018deepmimic %}.

<figure>
    <img width="70%" align="middle" src="/img/Notes/2024-10/reward_deepmimic.png" style="margin-top: 0px; margin-bottom: 5px"/>
    <div style="font-size: 12px; text-align: center; margin-top: 0px;">
    Reward function elements for sequence tracking in {% cite peng2020learning %}. The rewards are designed to encourage the robot to minimize the tracking errors on joint pose $\textbf{q}_t^{j}$, joint velocity $\dot{\textbf{q}}_t^{j}$, end-effector pose $\textbf{x}_t^{e}$, root pose $\textbf{x}_t^{r}$, and root velocity $\dot{\textbf{x}}_t^{r}$, respectively.
    </div>
</figure>

**Domain Adaptation:** 
Furthermore, to reduce the gap between simulation and the real world, the authors propose a domain adaptation method to encode the physical parameters $\mu$ with domain randomization $\mu \sim p(\mu)$. To prevent the encoder from overfitting and encourage the policy to be robust to physical uncertainty, they add the term as an information bottleneck like introduced in {% cite  alemi2016deep %} to the reward function as:

$$
    \arg\max_{\pi_\theta, E}
    \mathbb{E}_{\mu \sim p(\mu)}
    \mathbb{E}_{\textbf{z} \sim E(\textbf{z}|\mu)}
    \mathbb{E}_{\tau\sim p(\tau|\pi,\mu,\textbf{z})}
    [\sum_{t=0}^{T-1}\gamma^tr_t] 
    -\beta \mathbb{E}_{\mu \sim p(\mu)}[D_{KL}[E(\cdot|\mu)||\rho(\cdot)]]
$$

, where 
$E(\textbf{z}|\mu)$ 
is the physical parameter encoder, $\textbf{z}$ is the encoded physical parameters, $\rho(\cdot)$ is a variational prior for $\textbf{z}$, and $\tau\sim p(\tau|\pi,\mu,\textbf{z})$ are trajectories sampled from the policy $\pi_\theta$ with physical parameters $\mu$ and encoded vector $\textbf{z}$. Here $\beta$ can be treated as a Lagrangian multiplier to balance the information bottleneck and reward.

With the domain adaptation method, the robot shows a better performance in new environments especially for dynamic skills. The following figure shows the robot performing dog backward trotting learned by imitating backward played reference motions. Comparison results show that the adaptive policies outperform their non-adaptive counterparts in most skills.

<figure>
    <img width="70%" align="middle" src="/img/Notes/2024-10/mocap_to_robot.png" style="margin-top: 0px; margin-bottom: 5px"/>
    <div style="font-size: 12px; text-align: center; margin-top: 0px;">
    Comparison result. <b>Top</b>: Corresponding keypoints (red points) between dog skeleton (<b>Left</b>) and quadruped robot (<b>Right</b>). <b>Bottom</b>: Laikago robot performing dog backward trotting learned by imitating reference motions {% cite peng2020learning %}.
    </div>
</figure>


<!-- ---------------------------------------------------------------------------------------- -->
# References
{% bibliography --cited %}